AE_1.0
Basic AE models, trained twice with different datasplit seeds
All four models are dense

AE_1.1
Basic AE models, trained twice with different datasplit seeds
Three sparse BI models (sparse BA not possible)

AE_2.0
General idea: retrain the 4 AE models, now with ignoring unlinked genes in bi-models
merge_conditions = (1, 30, 50)
BI trained with MSE_masked, BA with regular MSE
No soft links
.0 patience = 5
.1 patience = 10, seed = 1
.2 patience = 10, seed = 2, track raw MSE
.3 patience = 10, seed = 3, track raw MSE
.4 patience = 10, seed = 4, track raw MSE
.5 patience = 10, seed = 5, track raw MSE
.6 patience = 10, seed = 6, track raw MSE
Job_ID      Model_name          Node        Time        Memory      Notes
11501987    AE_2.0.0_none       gpu26       00:08:15    1G          Trained with MSE_Masked
11502012    AE_2.0.0_encoder    gpu32       00:19:35    1.1G        -
11502013    AE_2.0.0_decoder    gpu33       02:03:18    1.1G        -
11507225    AE_2.0.0_none       influ1      00:08:16    909M        Trained with MSE
11506701    AE_2.0.0_both       influ2      --:--:--                Crashed...
11507675    AE_2.0.0_both       influ1      22:53:44    1.2G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572566    AE_2.0.1_none       gpu35       00:04:39    885.5M      -
11572567    AE_2.0.1_encoder    gpu35       00:45:12    1G          -
11572568    AE_2.0.1_decoder    gpu35       02:44:03    1.1G        -
11572569    AE_2.0.1_both       gpu35       20:56:18    1.1G        -
-------------------------------------------------------------------
11576395    AE_2.0.2_encoder    influ1      00:55:32    1.1G        -
11576396    AE_2.0.2_decoder    influ1      04:49:05    1.1G        -
11576397    AE_2.0.2_both       influ1      25:47:25    1.1G        -
11576398    AE_2.0.2_none       influ1      00:10:13    982.3M      -
-------------------------------------------------------------------
11578207    AE_2.0.3_encoder    influ3      00:41:09    1.1G        -
11578208    AE_2.0.3_decoder    influ2      05:09:11    1G          -
11578209    AE_2.0.3_both       insy15      30:00:24    1.2G        Timeout -> needs to be retrained with higher limit (2)
11578210    AE_2.0.3_none       influ2      00:11:16    904.5M      -
-------------------------------------------------------------------
11578686    AE_2.0.4_encoder    gpu35       00:26:38    1G          -
11578687    AE_2.0.4_decoder    influ1      05:36:13    1.1G        -
11578688    AE_2.0.4_both       gpu33       19:33:59    1.1G        -
11578689    AE_2.0.4_none       gpu32       00:03:44    894.2M      -
-------------------------------------------------------------------
11579221    AE_2.0.5_encoder    influ1      00:49:21    1.1G        -
11579222    AE_2.0.5_decoder    insy16      04:08:28    1.1G        -
11579223    AE_2.0.5_both       influ3      30:00:04    1.1G        Timeout -> needs to be retrained with higher limit (2)
11579224    AE_2.0.5_none       influ3      00:09:57    898.3M      -
-------------------------------------------------------------------
11579252    AE_2.0.6_encoder    gpu21       00:57:35    1019.4M     -
11579253    AE_2.0.6_decoder    influ3      04:39:59    1.1G        -
11579254    AE_2.0.6_both       gpu21       --:--:--    --          -
11579255    AE_2.0.6_none       gpu21       00:09:19    885.6M      -

AE_2.1
General idea: train the 3 bi-models, now with soft links
merge_conditions = (1, 30, 50)
no FC model (covered in AE_2.0)
Soft links and SL loss (not masked) soft_link_alpha=100
Job_ID      Model_name          Node        Time        Memory      Notes
11504174    AE_2.1.0_encoder    gpu26       00:22:47    1.1G        Needs to be redone because of faulty loss function
11504396    AE_2.1.0_decoder    gpu33       00:23:05    1.1G        Needs to be redone because of faulty loss function
11507049    AE_2.1.0_encoder    gpu34       00:34:02    1G          -
11507050    AE_2.1.0_decoder    gpu31       00:24:49    1G          -
11507051    AE_2.1.0_both       gpu31       01:44:58    1.2G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572571    AE_2.1.1_encoder    gpu35       00:34:09    1G          -
11572572    AE_2.1.1_decoder    gpu32       00:31:42    1G          -
11572573    AE_2.1.1_both       gpu32       02:58:21    1.1G        Check log for timeout -> Checked: no timeout
-------------------------------------------------------------------
11576399    AE_2.1.2_encoder    awi02       00:55:14    1.1G        -
11576400    AE_2.1.2_decoder    awi02       00:54:01    1.1G        -
11576401    AE_2.1.2_both       gpu30       02:04:02    1G          -
-------------------------------------------------------------------
11578211    AE_2.1.3_encoder    insy15      01:14:38    1.1G        -
11578212    AE_2.1.3_decoder    gpu35       00:32:06    1G          -
11578213    AE_2.1.3_both       gpu35       02:43:50    1.1G        -
-------------------------------------------------------------------
11578706    AE_2.1.4_encoder    gpu33       00:29:15    1G          -
11578707    AE_2.1.4_decoder    gpu33       00:35:49    1G          -
11578708    AE_2.1.4_both       gpu35       03:00:10    1.1G        Timeout -> needs to be retrained with higher limit
11579322    AE_2.1.4_both       influ2      02:28:57    1.1G        -
-------------------------------------------------------------------
11579225    AE_2.1.5_encoder    influ1      00:52:05    1.1G        -
11579226    AE_2.1.5_decoder    insy16      00:39:45    1.1G        -
11579227    AE_2.1.5_both       influ2      03:00:29    1.1G        Timeout -> needs to be retrained with higher limit
11579323    AE_2.1.5_both       influ3      04:00:01    1.1G        Timeout -> needs to be retrained with higher limit (2)
-------------------------------------------------------------------
11579256    AE_2.1.6_encoder    gpu21       00:53:30    1G          -
11579257    AE_2.1.6_decoder    gpu21       00:49:36    1G          -
11579258    AE_2.1.6_both       gpu21       03:00:04    --          Timeout -> needs to be retrained with higher limit (2)

AE_2.2
General idea: train the 3 bi-models, with randomized edge masks
merge_conditions = (1, 30, 50)
trained with MSE_Masked (unlinked genes remain unlinked)
no soft links
from .1 onwards: random_version = version = seed
Job_ID      Model_name          Node        Time        Memory      Notes
11507228    AE_2.2.0_encoder    gpu31       00:53:20    1G          Quite a bit longer than BI-FC GO...
11507229    AE_2.2.0_decoder    influ1      05:01:25    1G          -
11507230    AE_2.2.0_both       gpu30       20:14:38    1.1G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572574    AE_2.2.1_encoder    gpu32       00:45:12    1017.4M     -
11572575    AE_2.2.1_decoder    gpu33       03:39:34    1.1G        -
11572576    AE_2.2.1_both       gpu32       20:53:44    1.1G        -
-------------------------------------------------------------------
11576476    AE_2.2.2_encoder    gpu35       00:54:04    1G          -
11576477    AE_2.2.2_decoder    gpu35       02:27:40    1G          -
11576478    AE_2.2.2_both       gpu35       20:18:33    1.1G        -
-------------------------------------------------------------------
11578214    AE_2.2.3_encoder    gpu33       00:41:58    1020.1M     -
11578215    AE_2.2.3_decoder    gpu31       02:51:46    1G          -
11578216    AE_2.2.3_both       gpu32       20:17:28    1.1G        -
-------------------------------------------------------------------
11578838    AE_2.2.4_encoder    gpu32       00:50:10    1G          -
11578839    AE_2.2.4_decoder    insy16      03:26:42    1.1G        -
11578840    AE_2.2.4_both       insy16      30:00:21    1.2G        Timeout -> needs to be retrained with higher limit (2)
-------------------------------------------------------------------
11579228    AE_2.2.5_encoder    influ1      01:20:49    1.1G        -
11579229    AE_2.2.5_decoder    influ3      02:56:17    1019.7M     -
11579230    AE_2.2.5_both       influ2      11:01:50    1.1G        -
-------------------------------------------------------------------
11579259    AE_2.2.6_encoder    gpu21       01:08:28    1021.5M     -
11579260    AE_2.2.6_decoder    awi02       07:32:01    1.1G        -
11579261    AE_2.2.6_both       gpu30       20:32:30    1021.4M     -

AE_3.0
General idea: variable regularization on soft links
merge_conditions = (1, 30, 50)
Soft links and SL loss
.0 patience = 10
.1 patience = 100
.2 patience = 10000, n_epochs = 1000
.3 patience = 10000, n_epochs = 1000, now tracking MSE without regularization
alpha = 100
Job_ID      Model_name          Node        Time        Memory      Notes
11560162    AE_3.0.0_encoder    awi02       00:46:25    1.1G        Too low patience
11560163    AE_3.0.0_decoder    gpu07       01:09:57    1G          Too low patience
11560194    AE_3.0.0_encoder    gpu07       00:43:21    1G          -
11560195    AE_3.0.0_decoder    gpu07       00:38:24    1G          -
-------------------------------------------------------------------
11562237    AE_3.0.1_encoder    gpu32       01:06:49    1G          -
11562238    AE_3.0.1_decoder    gpu30       01:25:41    1G          -
-------------------------------------------------------------------
11562316    AE_3.0.2_encoder    gpu33       01:07:25    1G          -
11562317    AE_3.0.2_decoder    gpu31       01:16:22    1G          -
-------------------------------------------------------------------
11575901    AE_3.0.3_encoder    influ1      02:10:09    1.1G        -
11575902    AE_3.0.3_decoder    influ1      02:24:31    1.1G        -

AE_3.1
alpha = 1000
Job_ID      Model_name          Node        Time        Memory      Notes
11560167    AE_3.1.0_encoder    gpu12       00:13:57    1G          Too low patience
11560168    AE_3.1.0_decoder    gpu15       00:01:34    1G          Too low patience
11560196    AE_3.1.0_encoder    gpu12       00:36:32    1G          -
11560197    AE_3.1.0_decoder    gpu07       00:58:43    1G          -
-------------------------------------------------------------------
11562318    AE_3.1.2_encoder    gpu31       01:05:12    1G          -
11562319    AE_3.1.2_decoder    gpu21       01:41:50    1G          -
-------------------------------------------------------------------
11575903    AE_3.1.3_encoder    influ1      02:12:51    1.1G        -
11575904    AE_3.1.3_decoder    influ1      02:29:54    1.1G        -

AE_3.2
alpha = 10000
Job_ID      Model_name          Node        Time        Memory      Notes
11560184    AE_3.2.0_encoder    gpu07       00:01:39    1G          Too low patience
11560173    AE_3.2.0_decoder    gpu18       00:01:08    1G          Too low patience
11560198    AE_3.2.0_encoder    gpu12       00:22:46    1G          -
11560199    AE_3.2.0_decoder    gpu32       00:03:03    1G          Too low patience
-------------------------------------------------------------------
11562320    AE_3.2.2_encoder    gpu30       01:05:19    1G          -
11562321    AE_3.2.2_decoder    gpu31       01:13:05    1G          -
-------------------------------------------------------------------
11575905    AE_3.2.3_encoder    influ2      02:01:39    1.1G        -
11575906    AE_3.2.3_decoder    influ2      02:15:58    1.1G        -

AE_3.3
alpha = 100000
Job_ID      Model_name          Node        Time        Memory      Notes
11560200    AE_3.3.0_encoder    gpu09       00:01:49    1G          Too low patience
11560211    AE_3.3.0_encoder    gpu31       00:01:50    1G          Still too fast... (good example for Timo) -> increase patience!!
11560201    AE_3.3.0_decoder    -----       00:01:49    1G          Too low patience
-------------------------------------------------------------------
11562239    AE_3.3.1_encoder    gpu33       00:07:16    1G          -
11562240    AE_3.3.1_decoder    -----       00:09:56    1G          Too low patience
-------------------------------------------------------------------
11562322    AE_3.3.2_encoder    gpu33       01:03:10    1G          -
11562323    AE_3.3.2_decoder    gpu31       01:14:48    1G          -
-------------------------------------------------------------------
11575908    AE_3.3.3_encoder    insy16      01:57:30    1.1G        -
11575907    AE_3.3.3_decoder    insy15      02:09:55    1.1G        -

AE_3.-1
Baselines for Fixed Links and Fully Connected, also trained for 1000 epochs instead of based on patience
Fixed Links trained with mse masked, FC trained with mse
.3 MSE_L1, a = 100
Job_ID      Model_name          Node        Time        Memory      Notes
11571959    AE_3.-1.2_encoder   gpu32       01:00:15	1G          -
11571962    AE_3.-1.2_decoder   gpu32       01:12:01    1G          -
11571964    AE_3.-1.2_none      gpu32       00:08:25    890.7M      -
------------------------------------------------------------------- FC trained with L1 regularization, a = 100
11576225    AE_3.-1.3_none      insy16      00:21:37    979.2M      -

AE_3.4
General idea: FC with L1 regularization
merge_conditions = (1, 30, 50)
Regular MSE loss
Job_ID      Model_name          Node        Time        Memory      Notes