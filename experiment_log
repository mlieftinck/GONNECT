AE_1.0
Basic AE models, trained twice with different datasplit seeds
All four models are dense

AE_1.1
Basic AE models, trained twice with different datasplit seeds
Three sparse BI models (sparse BA not possible)

AE_2.0
General idea: retrain the 4 AE models, now with ignoring unlinked genes in bi-models
merge_conditions = (1, 30, 50)
BI trained with MSE_masked, BA with regular MSE
No soft links
.0 patience = 5
.1 patience = 10, seed = 1
.2 patience = 10, seed = 2
Job_ID      Model_name          Node        Time        Memory      Notes
11501987    AE_2.0.0_none       gpu26       00:08:15    1G          Trained with MSE_Masked
11502012    AE_2.0.0_encoder    gpu32       00:19:35    1.1G        -
11502013    AE_2.0.0_decoder    gpu33       02:03:18    1.1G        -
11507225    AE_2.0.0_none       influ1      00:08:16    909M        Trained with MSE
11506701    AE_2.0.0_both       influ2      --:--:--                Crashed...
11507675    AE_2.0.0_both       influ1      22:53:44    1.2G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572566    AE_2.0.1_none       gpu35       00:04:39    885.5M      -
11572567    AE_2.0.1_encoder    gpu35       00:45:12    1G          -
11572568    AE_2.0.1_decoder    gpu35       02:44:03    1.1G        -
11572569    AE_2.0.1_both       gpu35       20:56:18    1.1G        -
-------------------------------------------------------------------
--------    AE_2.2.2_encoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_decoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_both       gpu--       --:--:--    --          -

AE_2.1
General idea: train the 3 bi-models, now with soft links
merge_conditions = (1, 30, 50)
no FC model (covered in AE_2.0)
Soft links and SL loss (not masked) soft_link_alpha=100
Job_ID      Model_name          Node        Time        Memory      Notes
11504174    AE_2.1.0_encoder    gpu26       00:22:47    1.1G        Needs to be redone because of faulty loss function
11504396    AE_2.1.0_decoder    gpu33       00:23:05    1.1G        Needs to be redone because of faulty loss function
11507049    AE_2.1.0_encoder    gpu34       00:34:02    1G          -
11507050    AE_2.1.0_decoder    gpu31       00:24:49    1G          -
11507051    AE_2.1.0_both       gpu31       01:44:58    1.2G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572571    AE_2.1.1_encoder    gpu35       00:34:09    1G          -
11572572    AE_2.1.1_decoder    gpu32       00:31:42    1G          -
11572573    AE_2.1.1_both       gpu32       02:58:21    1.1G        Check log for timeout
-------------------------------------------------------------------
--------    AE_2.2.2_encoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_decoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_both       gpu--       --:--:--    --          -

AE_2.2
General idea: train the 3 bi-models, with randomized edge masks
merge_conditions = (1, 30, 50)
trained with MSE_Masked (unlinked genes remain unlinked)
no soft links
Job_ID      Model_name          Node        Time        Memory      Notes
11507228    AE_2.2.0_encoder    gpu31       00:53:20    1G          Quite a bit longer than BI-FC GO...
11507229    AE_2.2.0_decoder    influ1      05:01:25    1G          -
11507230    AE_2.2.0_both       gpu30       20:14:38    1.1G        -
------------------------------------------------------------------- Rerun all AE_2 experiments to test robustness (with increased patience)
11572574    AE_2.2.1_encoder    gpu32       00:45:12    1017.4M     -
11572575    AE_2.2.1_decoder    gpu33       03:39:34    1.1G        -
11572576    AE_2.2.1_both       gpu32       20:53:44    1.1G        -
-------------------------------------------------------------------
--------    AE_2.2.2_encoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_decoder    gpu--       --:--:--    --          -
--------    AE_2.2.2_both       gpu--       --:--:--    --          -

AE_3.0
General idea: variable regularization on soft links
merge_conditions = (1, 30, 50)
Soft links and SL loss
.0 patience = 10
.1 patience = 100
.2 patience = 10000, n_epochs = 1000
.3 patience = 10000, n_epochs = 1000
alpha = 100
Job_ID      Model_name          Node        Time        Memory      Notes
11560162    AE_3.0.0_encoder    awi02       00:46:25    1.1G        Too low patience
11560163    AE_3.0.0_decoder    gpu07       01:09:57    1G          Too low patience
11560194    AE_3.0.0_encoder    gpu07       00:43:21    1G          -
11560195    AE_3.0.0_decoder    gpu07       00:38:24    1G          -
-------------------------------------------------------------------
11562237    AE_3.0.1_encoder    gpu32       01:06:49    1G          -
11562238    AE_3.0.1_decoder    gpu30       01:25:41    1G          -
-------------------------------------------------------------------
11562316    AE_3.0.2_encoder    gpu33       01:07:25    1G          -
11562317    AE_3.0.2_decoder    gpu31       01:16:22    1G          -
-------------------------------------------------------------------
--------    AE_3.0.3_encoder    gpu--       --:--:--    --          -
--------    AE_3.0.3_decoder    gpu--       --:--:--    --          -

AE_3.1
alpha = 1000
Job_ID      Model_name          Node        Time        Memory      Notes
11560167    AE_3.1.0_encoder    gpu12       00:13:57    1G          Too low patience
11560168    AE_3.1.0_decoder    gpu15       00:01:34    1G          Too low patience
11560196    AE_3.1.0_encoder    gpu12       00:36:32    1G          -
11560197    AE_3.1.0_decoder    gpu07       00:58:43    1G          -
-------------------------------------------------------------------
11562318    AE_3.1.2_encoder    gpu31       01:05:12    1G          -
11562319    AE_3.1.2_decoder    gpu21       01:41:50    1G          -
-------------------------------------------------------------------
--------    AE_3.1.3_encoder    gpu--       --:--:--    --          -
--------    AE_3.1.3_decoder    gpu--       --:--:--    --          -

AE_3.2
alpha = 10000
Job_ID      Model_name          Node        Time        Memory      Notes
11560184    AE_3.2.0_encoder    gpu07       00:01:39    1G          Too low patience
11560173    AE_3.2.0_decoder    -----       00:01:08    1G          Too low patience
11560198    AE_3.2.0_encoder    gpu12       00:22:46    1G          -
11560199    AE_3.2.0_decoder    -----       00:03:03    1G          Too low patience
-------------------------------------------------------------------
11562320    AE_3.2.2_encoder    gpu30       01:05:19    1G          -
11562321    AE_3.2.2_decoder    gpu31       01:13:05    1G          -
-------------------------------------------------------------------
--------    AE_3.2.3_encoder    gpu--       --:--:--    --          -
--------    AE_3.2.3_decoder    gpu--       --:--:--    --          -

AE_3.3
alpha = 100000
Job_ID      Model_name          Node        Time        Memory      Notes
11560200    AE_3.3.0_encoder    gpu09       00:01:49    1G          Too low patience
11560211    AE_3.3.0_encoder    gpu31       00:01:50    1G          Still too fast... (good example for Timo) -> increase patience!!
11560201    AE_3.3.0_decoder    -----       00:01:49    1G          Too low patience
-------------------------------------------------------------------
11562239    AE_3.3.1_encoder    gpu33       00:07:16    1G          -
11562240    AE_3.3.1_decoder    -----       00:09:56    1G          Too low patience
-------------------------------------------------------------------
11562322    AE_3.3.2_encoder    gpu33       01:03:10    1G          -
11562323    AE_3.3.2_decoder    gpu31       01:14:48    1G          -
-------------------------------------------------------------------
--------    AE_3.3.3_encoder    gpu--       --:--:--    --          -
--------    AE_3.3.3_decoder    gpu--       --:--:--    --          -

AE_3.-1
Baselines for Fixed Links and Fully Connected, also trained for 1000 epochs instead of based on patience
Fixed Links trained with mse masked, FC trained with mse
Job_ID      Model_name          Node        Time        Memory      Notes
11571959    AE_3.-1.2_encoder   gpu32       01:00:15	1G          -
11571962    AE_3.-1.2_decoder   gpu32       01:12:01    1G          -
11571964    AE_3.-1.2_none      gpu32       00:08:25    890.7M      -
------------------------------------------------------------------- FC trained with L1 regularization, a = ...
--------    AE_3.-1.3_both    gpu--       --:--:--    --          -

AE_3.4
General idea: FC with L1 regularization
merge_conditions = (1, 30, 50)
Regular MSE loss
Job_ID      Model_name          Node        Time        Memory      Notes